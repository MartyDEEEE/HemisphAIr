import os
import subprocess
import requests
import json
import time
import threading
import sys
import re

class Terminal:
    """Class to handle terminal output with thread safety"""
    def __init__(self, name):
        self.name = name
        self.buffer = ""
        self.lock = threading.Lock()
        self.subprocess = None
        self.is_external = False
        self.command_history = []
        
    def write(self, text):
        with self.lock:
            self.buffer += text
            sys.stdout.write(text)
            sys.stdout.flush()
    
    def clear(self):
        with self.lock:
            self.buffer = ""
            
    def get_buffer(self):
        with self.lock:
            return self.buffer
    
    def launch_external_terminal(self):
        """Launch an external terminal that llm1 can directly interact with"""
        # Create a dedicated directory for this terminal session
        terminal_dir = f"{self.name.replace(' ', '_').lower()}_terminal"
        if not os.path.exists(terminal_dir):
            os.makedirs(terminal_dir)
            
        # Create a Python script that will run in the terminal
        terminal_script = os.path.join(terminal_dir, "terminal_interface.py")
        
        with open(terminal_script, "w") as f:
            f.write("""
import os
import sys
import time
import socket
import threading
import subprocess
import json
import traceback

# Set up a server socket to communicate with the main process
server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server_socket.bind(('localhost', 0))  # Use any available port
server_port = server_socket.getsockname()[1]
server_socket.listen(1)

print(f"TERMINAL_PORT={server_port}")
sys.stdout.flush()

def handle_client(conn):
    while True:
        try:
            # Receive command from the main process
            data = conn.recv(4096).decode('utf-8')
            
            if not data:
                break
                
            if data.startswith("EXIT_TERMINAL"):
                print("\\nExiting terminal...")
                conn.close()
                sys.exit(0)
                
            # Parse the command
            try:
                command_data = json.loads(data)
                command_type = command_data.get('type', '')
                command = command_data.get('command', '')
                
                if command_type == 'shell':
                    # Execute shell command
                    print(f"\\n$ {command}")
                    sys.stdout.flush()
                    
                    process = subprocess.Popen(
                        command,
                        shell=True,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        bufsize=1
                    )
                    
                    # Capture output in real-time
                    stdout_data = ""
                    stderr_data = ""
                    
                    for line in process.stdout:
                        stdout_data += line
                        print(line, end='')
                        sys.stdout.flush()
                        
                    process.stdout.close()
                    
                    for line in process.stderr:
                        stderr_data += line
                        print(f"Error: {line}", end='')
                        sys.stdout.flush()
                        
                    process.stderr.close()
                    
                    return_code = process.wait()
                    
                    response = {
                        'stdout': stdout_data,
                        'stderr': stderr_data,
                        'return_code': return_code
                    }
                    
                    # Send the response back to the main process
                    conn.sendall(json.dumps(response).encode('utf-8'))
                    
                elif command_type == 'python':
                    # Execute Python code
                    try:
                        # Save the code to a file
                        with open("temp_code.py", "w") as f:
                            f.write(command)
                            
                        print(f"\\n>>> Executing Python code:")
                        
                        # Print the code
                        for i, line in enumerate(command.split('\\n')):
                            print(f"{i+1}: {line}")
                            
                        print("\\nOutput:")
                        sys.stdout.flush()
                        
                        # Execute the code
                        process = subprocess.Popen(
                            [sys.executable, "temp_code.py"],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=True,
                            bufsize=1
                        )
                        
                        # Capture output in real-time
                        stdout_data = ""
                        stderr_data = ""
                        
                        for line in process.stdout:
                            stdout_data += line
                            print(line, end='')
                            sys.stdout.flush()
                            
                        process.stdout.close()
                        
                        for line in process.stderr:
                            stderr_data += line
                            print(f"Error: {line}", end='')
                            sys.stdout.flush()
                            
                        process.stderr.close()
                        
                        return_code = process.wait()
                        
                        print(f"\\n>>> Process exited with code {return_code}")
                        sys.stdout.flush()
                        
                        response = {
                            'stdout': stdout_data,
                            'stderr': stderr_data,
                            'return_code': return_code
                        }
                        
                        # Send the response back to the main process
                        conn.sendall(json.dumps(response).encode('utf-8'))
                        
                    except Exception as e:
                        error_traceback = traceback.format_exc()
                        print(f"Error executing Python code: {str(e)}\\n{error_traceback}")
                        sys.stdout.flush()
                        
                        response = {
                            'stdout': '',
                            'stderr': f"Error: {str(e)}\\n{error_traceback}",
                            'return_code': -1
                        }
                        
                        # Send the response back to the main process
                        conn.sendall(json.dumps(response).encode('utf-8'))
                        
            except json.JSONDecodeError:
                print(f"Error parsing command: {data}")
                sys.stdout.flush()
                
                response = {
                    'stdout': '',
                    'stderr': f"Error parsing command: {data}",
                    'return_code': -1
                }
                
                # Send the response back to the main process
                conn.sendall(json.dumps(response).encode('utf-8'))
                
        except Exception as e:
            error_traceback = traceback.format_exc()
            print(f"Error in terminal interface: {str(e)}\\n{error_traceback}")
            sys.stdout.flush()
            break
    
    conn.close()

# Accept a connection from the main process
print("Waiting for connection from main process...")
sys.stdout.flush()

client_socket, _ = server_socket.accept()
print("Connected to main process")
sys.stdout.flush()

handle_client(client_socket)

print("Terminal interface shutting down")
server_socket.close()
sys.exit(0)
""")
        
        # Determine the appropriate terminal command based on OS
        terminal_command = ""
        if sys.platform.startswith('win'):
            # Windows: use 'start' to open a new Command Prompt window
            terminal_command = f'start cmd /K python "{terminal_script}"'
        elif sys.platform.startswith('darwin'):
            # macOS: use 'open' with Terminal.app
            terminal_command = f'osascript -e \'tell app "Terminal" to do script "cd {os.getcwd()} && python3 {terminal_script}"\''
        else:
            # Linux/Unix: try common terminal emulators
            terminals = ['gnome-terminal', 'xterm', 'konsole', 'terminology', 'terminator']
            for term in terminals:
                if subprocess.call(['which', term], stdout=subprocess.PIPE, stderr=subprocess.PIPE) == 0:
                    if term == 'gnome-terminal':
                        terminal_command = f'{term} -- python3 "{terminal_script}"'
                    else:
                        terminal_command = f'{term} -e "python3 \'{terminal_script}\'"'
                    break
        
        if not terminal_command:
            print(f"Could not find a suitable terminal emulator for your system. Using fallback method.")
            return False
            
        try:
            # Launch the terminal with our script
            print(f"Launching external terminal for {self.name}...")
            process = subprocess.Popen(
                terminal_command, 
                shell=True, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # Wait for the terminal to start and extract the port number
            port_line = ""
            for line in process.stdout:
                if line.startswith("TERMINAL_PORT="):
                    port_line = line.strip()
                    break
            
            if not port_line:
                print(f"Error: Terminal did not report its port. Falling back to internal terminal.")
                return False
                
            # Extract the port number
            port = int(port_line.split("=")[1])
            
            # Connect to the terminal
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.connect(('localhost', port))
            
            self.is_external = True
            self.subprocess = process
            
            print(f"Successfully connected to external terminal for {self.name}.")
            return True
            
        except Exception as e:
            print(f"Error launching external terminal: {str(e)}")
            print(f"Falling back to internal terminal.")
            return False
    
    def send_command(self, command_type, command):
        """Send a command to the external terminal"""
        if not self.is_external:
            return {
                'stdout': '',
                'stderr': 'External terminal not available',
                'return_code': -1
            }
            
        try:
            # Format the command as JSON
            command_data = {
                'type': command_type,
                'command': command
            }
            
            # Add to command history
            self.command_history.append(command_data)
            
            # Send the command to the terminal
            self.socket.sendall(json.dumps(command_data).encode('utf-8'))
            
            # Receive the response
            response_data = ""
            while True:
                data = self.socket.recv(4096).decode('utf-8')
                if not data:
                    break
                    
                response_data += data
                
                try:
                    # Try to parse as JSON to see if it's complete
                    response = json.loads(response_data)
                    return response
                except json.JSONDecodeError:
                    # Not complete yet, keep receiving
                    continue
                    
            # If we got here, something went wrong
            return {
                'stdout': '',
                'stderr': 'Incomplete response from terminal',
                'return_code': -1
            }
            
        except Exception as e:
            return {
                'stdout': '',
                'stderr': f'Error sending command to terminal: {str(e)}',
                'return_code': -1
            }
    
    def close_external_terminal(self):
        """Close the external terminal"""
        if not self.is_external:
            return
            
        try:
            # Send exit command to the terminal
            self.socket.sendall("EXIT_TERMINAL".encode('utf-8'))
            self.socket.close()
            
            # Wait for the subprocess to exit
            if self.subprocess:
                self.subprocess.wait(timeout=2)
                
        except Exception as e:
            print(f"Error closing external terminal: {str(e)}")
            
        self.is_external = False
        self.subprocess = None

class LLM:
    """Class to represent an LLM with methods for getting responses and executing code"""
    def __init__(self, model_name, role, terminal, can_execute_code=False, use_external_terminal=False):
        self.model_name = model_name
        self.role = role
        self.terminal = terminal
        self.can_execute_code = can_execute_code
        self.use_external_terminal = use_external_terminal
        self.conversation_history = []
        
        # If this LLM should use an external terminal, try to launch it
        if self.use_external_terminal and self.can_execute_code:
            self.has_external_terminal = self.terminal.launch_external_terminal()
        else:
            self.has_external_terminal = False
        
    def get_response_stream(self, prompt, callback=None):
        """Get a response from the LLM and stream it in real-time"""
        url = "http://localhost:11434/api/generate"
        
        # Add more details to the request to help Ollama identify the model
        data = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
        
        try:
            response = requests.post(url, json=data, stream=True)
            
            if response.status_code == 200:
                full_response = ""
                for line in response.iter_lines():
                    if line:
                        try:
                            line_json = json.loads(line.decode('utf-8'))
                            response_piece = line_json.get("response", "")
                            full_response += response_piece
                            
                            if callback:
                                callback(response_piece)
                            
                            # Check if this is the end of the response
                            if line_json.get("done", False):
                                break
                        except json.JSONDecodeError:
                            # Handle potential JSON decoding errors
                            if callback:
                                callback(f"Error decoding response: {line.decode('utf-8')}")
                
                # Add to conversation history
                self.conversation_history.append({"role": "user", "content": prompt})
                self.conversation_history.append({"role": "assistant", "content": full_response})
                
                return full_response
            else:
                error_message = f"Error: {response.status_code} - {response.text}"
                
                if callback:
                    callback(error_message)
                    
                return error_message
        except requests.exceptions.RequestException as e:
            error_message = f"Error connecting to Ollama: {str(e)}"
            
            if callback:
                callback(error_message)
                
            return error_message
    
    def extract_code_blocks(self, text):
        """Extract Python code blocks from text"""
        code_blocks = []
        pattern = r"```python(.*?)```"
        matches = re.findall(pattern, text, re.DOTALL)
        
        for match in matches:
            code_blocks.append(match.strip())
            
        return code_blocks
    
    def execute_code(self, code, callback=None):
        """Execute Python code and capture the output"""
        if not self.can_execute_code:
            message = "This LLM is not allowed to execute code."
            if callback:
                callback(message)
            return {
                "stdout": "",
                "stderr": message,
                "return_code": -1,
                "status": "error"
            }
        
        # If using an external terminal, send the code there
        if self.has_external_terminal:
            if callback:
                callback("Sending code to external terminal...\n")
            
            # Send the command to the external terminal
            result = self.terminal.send_command('python', code)
            
            # Process the result
            if callback:
                if 'stdout' in result and result['stdout']:
                    callback(result['stdout'])
                
                if 'stderr' in result and result['stderr']:
                    callback(f"Error: {result['stderr']}")
                
                return_code = result.get('return_code', -1)
                if return_code != 0:
                    callback(f"\nCode execution failed with return code {return_code}")
                    status = "failed"
                else:
                    callback("\nCode execution completed successfully")
                    status = "success"
            
            # Return the result
            result["status"] = status
            return result
        
        # Otherwise, use the internal execution method
        # Create a directory for the LLM if it doesn't exist
        llm_dir = f"{self.role.replace(' ', '_').lower()}_workspace"
        if not os.path.exists(llm_dir):
            os.makedirs(llm_dir)
        
        # Change to the LLM's directory
        current_dir = os.getcwd()
        os.chdir(llm_dir)
        
        # Save the code to a temporary file
        try:
            with open("temp_code.py", "w") as f:
                f.write(code)
        except Exception as e:
            if callback:
                callback(f"Error writing code to file: {str(e)}")
            os.chdir(current_dir)  # Make sure to change back to the original directory
            return {
                "stdout": "",
                "stderr": f"Error writing code to file: {str(e)}",
                "return_code": -1,
                "status": "error"
            }
        
        # Execute the code and capture the output
        try:
            if callback:
                callback("Executing code...\n")
            
            # Create a process to run the code with a timeout
            process = subprocess.Popen(["python", "temp_code.py"], 
                                      stdout=subprocess.PIPE, 
                                      stderr=subprocess.PIPE, 
                                      text=True)
            
            stdout_lines = []
            stderr_lines = []
            
            # Set a timeout for the code execution
            max_execution_time = 30  # seconds
            start_time = time.time()
            
            # Read stdout and stderr in real-time
            while True:
                # Check if we've exceeded the timeout
                if time.time() - start_time > max_execution_time:
                    process.kill()
                    if callback:
                        callback("\nCode execution timed out after 30 seconds.")
                    return {
                        "stdout": "".join(stdout_lines),
                        "stderr": "Execution timed out after 30 seconds.",
                        "return_code": -1,
                        "status": "timeout"
                    }
                
                # Try to read a line from stdout and stderr
                try:
                    stdout_line = process.stdout.readline()
                    stderr_line = process.stderr.readline()
                    
                    if stdout_line:
                        stdout_lines.append(stdout_line)
                        if callback:
                            callback(stdout_line)
                    
                    if stderr_line:
                        stderr_lines.append(stderr_line)
                        if callback:
                            callback(f"Error: {stderr_line}")
                    
                    # Check if the process has finished
                    if process.poll() is not None:
                        # Read any remaining output
                        for line in process.stdout:
                            stdout_lines.append(line)
                            if callback:
                                callback(line)
                        
                        for line in process.stderr:
                            stderr_lines.append(line)
                            if callback:
                                callback(f"Error: {line}")
                        
                        break
                    
                    # Sleep briefly to avoid high CPU usage
                    time.sleep(0.01)
                
                except Exception as e:
                    if callback:
                        callback(f"\nError reading process output: {str(e)}")
                    stderr_lines.append(f"Error reading process output: {str(e)}")
                    break
            
            return_code = process.returncode
            
            stdout_text = "".join(stdout_lines)
            stderr_text = "".join(stderr_lines)
            
            result = {
                "stdout": stdout_text,
                "stderr": stderr_text,
                "return_code": return_code
            }
            
            if return_code != 0:
                if callback:
                    callback(f"\nCode execution failed with return code {return_code}")
                result["status"] = "failed"
            else:
                if callback:
                    callback("\nCode execution completed successfully")
                result["status"] = "success"
            
            return result
            
        except Exception as e:
            if callback:
                callback(f"\nError executing code: {str(e)}")
            return {
                "stdout": "",
                "stderr": str(e),
                "return_code": -1,
                "status": "error"
            }
        finally:
            # Clean up and return to the original directory
            os.chdir(current_dir)

class BrainSimulation:
    """Main class that orchestrates the conversation between the LLMs"""
    def __init__(self, llm1_model="llama2", llm2_model="mistral", use_external_terminal=True):
        # Initialize terminals
        self.llm1_terminal = Terminal("Left Hemisphere")
        self.llm2_terminal = Terminal("Right Hemisphere")
        
        # Initialize LLMs
        self.llm1 = LLM(llm1_model, "Left Hemisphere", self.llm1_terminal, 
                        can_execute_code=True, use_external_terminal=use_external_terminal)
        self.llm2 = LLM(llm2_model, "Right Hemisphere", self.llm2_terminal, 
                        can_execute_code=False)
    
    def run(self):
        """Run the brain simulation"""
        print("Welcome to the Hemispherical Brain Simulation!")
        print("Type 'exit' to quit the simulation.")
        
        # Check if we have an external terminal for llm1
        if self.llm1.has_external_terminal:
            print("Left Hemisphere (llm1) has its own terminal window.")
            print("It can execute Python code and shell commands in that window.")
        else:
            print("Left Hemisphere (llm1) will execute code within this process.")
        
        # Check if Ollama is running
        try:
            response = requests.get("http://localhost:11434/api/version")
            if response.status_code != 200:
                print("Error: Could not connect to Ollama server. Make sure it's running on http://localhost:11434")
                return
        except requests.exceptions.RequestException:
            print("Error: Could not connect to Ollama server. Make sure it's running on http://localhost:11434")
            return
        
        # Check if the models are available
        try:
            # The correct API endpoint for listing models
            models_response = requests.get("http://localhost:11434/api/tags")
            if models_response.status_code != 200:
                print("Error: Could not get the list of available models from Ollama.")
                return
            
            # Debug: Print raw response to understand the structure
            models_data = models_response.json()
            print("Available models in Ollama:")
            
            # The response format seems to have changed in newer Ollama versions
            if "models" in models_data:
                # Older format
                available_models = [model["name"] for model in models_data.get("models", [])]
                for model in available_models:
                    print(f"  - {model}")
            else:
                # Newer format - direct list of models
                available_models = []
                for model_info in models_data.get("models", []):
                    model_name = model_info.get("name", "")
                    if model_name:
                        available_models.append(model_name)
                        print(f"  - {model_name}")
            
            # If no models were found, try a different approach - list them directly
            if not available_models:
                print("No models found with the first method. Trying alternative approach...")
                list_response = requests.post("http://localhost:11434/api/list")
                if list_response.status_code == 200:
                    models_list = list_response.json().get("models", [])
                    available_models = [model.get("name", "") for model in models_list]
                    print("Models found with alternative method:")
                    for model in available_models:
                        print(f"  - {model}")
            
            # If still no models, skip the check
            if not available_models:
                print("Warning: Could not determine available models. Proceeding anyway...")
                return
            
            # Check if specified models are available, with more permissive matching
            model1_available = any(self.llm1.model_name.lower() in model.lower() for model in available_models)
            model2_available = any(self.llm2.model_name.lower() in model.lower() for model in available_models)
            
            if not model1_available:
                print(f"Warning: Model matching '{self.llm1.model_name}' was not found in available models.")
                print(f"Would you like to continue anyway? (y/n)")
                response = input().lower()
                if response != 'y':
                    return
                
            if not model2_available:
                print(f"Warning: Model matching '{self.llm2.model_name}' was not found in available models.")
                print(f"Would you like to continue anyway? (y/n)")
                response = input().lower()
                if response != 'y':
                    return
                
        except requests.exceptions.RequestException as e:
            print(f"Error checking available models: {str(e)}")
            print("Would you like to continue anyway? (y/n)")
            response = input().lower()
            if response != 'y':
                return
        except json.JSONDecodeError as e:
            print(f"Error parsing the response from Ollama: {str(e)}")
            print("Would you like to continue anyway? (y/n)")
            response = input().lower()
            if response != 'y':
                return
        
        print(f"Using models: {self.llm1.model_name} (Left Hemisphere) and {self.llm2.model_name} (Right Hemisphere)")
        
        while True:
            try:
                # Get the user's query
                user_query = input("\nYou: ")
                
                if user_query.lower() == 'exit':
                    print("Exiting the simulation. Goodbye!")
                    break
                
                # Run a conversation between the LLMs
                self.run_conversation(user_query)
            except KeyboardInterrupt:
                print("\nKeyboard interrupt detected. Exiting the simulation. Goodbye!")
                break
            except Exception as e:
                print(f"\nAn error occurred: {str(e)}")
                print("The simulation will continue with the next query.")
    
    def run_conversation(self, user_query):
        """Run a conversation between the LLMs based on the user's query"""
        # Construct the prompt for llm1
        llm1_prompt = f"""
        You are simulating the left hemisphere of a brain in a hemispherical brain model. 
        
        Left hemisphere characteristics:
        - Analytical and logical thinking
        - Sequential and linear processing
        - Detail-oriented
        - Language processing
        - Mathematical reasoning
        
        A user has asked: "{user_query}"
        
        Think about this query from your left-hemisphere perspective and discuss it with the right hemisphere (llm2).
        You have the ability to execute Python code to help solve problems or demonstrate concepts.
        
        {'You have your own dedicated terminal window where your code will run. You can see the output in real-time in that window.' if self.llm1.has_external_terminal else 'Your code will run in this process and the output will be shown here.'}
        
        To execute code, format it as:
        ```python
        # Your code here
        ```
        
        Be concise but thorough in your analysis. Focus on facts, logic, and detailed analysis.
        """
        
        # Clear the terminals
        self.llm1_terminal.clear()
        self.llm2_terminal.clear()
        
        print("\nLeft Hemisphere (llm1):", end="\n", flush=True)
        
        # Get llm1's response
        llm1_response = self.llm1.get_response_stream(llm1_prompt, self.llm1_terminal.write)
        
        # Extract and execute code blocks
        code_blocks = self.llm1.extract_code_blocks(llm1_response)
        execution_results = []
        
        for i, code_block in enumerate(code_blocks):
            print(f"\nExecuting code block {i+1}:")
            print("```python")
            print(code_block)
            print("```")
            
            result = self.llm1.execute_code(code_block, lambda text: sys.stdout.write(text) or sys.stdout.flush())
            execution_results.append(result)
        
        # Construct a summary of the code execution results
        execution_summary = ""
        for i, result in enumerate(execution_results):
            execution_summary += f"\nCode Block {i+1} Execution Result:\n"
            execution_summary += f"Status: {result['status']}\n"
            
            if result['stdout']:
                execution_summary += f"Output:\n{result['stdout']}\n"
            
            if result['stderr']:
                execution_summary += f"Errors:\n{result['stderr']}\n"
        
        # Construct the prompt for llm2
        llm2_prompt = f"""
        You are simulating the right hemisphere of a brain in a hemispherical brain model.
        
        Right hemisphere characteristics:
        - Intuitive and holistic thinking
        - Parallel processing
        - Big-picture perspective
        - Visual and spatial awareness
        - Emotional processing
        - Creative thinking
        
        A user has asked: "{user_query}"
        
        The left hemisphere (llm1) has responded:
        "{llm1_response}"
        
        {execution_summary if execution_results else ""}
        
        What are your thoughts on this question from your right-hemisphere perspective? Consider the emotional, intuitive, and creative aspects that the left hemisphere might have missed. You can provide alternative perspectives or expand on the ideas.
        
        Be concise but insightful in your response. Focus on creativity, intuition, emotion, and holistic understanding.
        """
        
        print("\nRight Hemisphere (llm2):", end="\n", flush=True)
        
        # Get llm2's response
        llm2_response = self.llm2.get_response_stream(llm2_prompt, self.llm2_terminal.write)
        
        # Now let llm1 respond to llm2
        llm1_followup_prompt = f"""
        The right hemisphere (llm2) has responded to the user's query:
        "{llm2_response}"
        
        What are your final thoughts after considering their perspective? Try to integrate their insights with your analytical approach to provide a complete understanding of the topic.
        
        Remember to stay true to your left-hemisphere characteristics (analytical, logical, sequential, detail-oriented), but acknowledge the value of the right-hemisphere's contribution.
        
        Be concise and focus on the most important insights.
        """
        
        print("\nLeft Hemisphere (llm1) Final Thoughts:", end="\n", flush=True)
        
        # Get llm1's final response
        llm1_final_response = self.llm1.get_response_stream(llm1_followup_prompt, self.llm1_terminal.write)
        
        # Extract and execute any additional code blocks from the final response
        code_blocks = self.llm1.extract_code_blocks(llm1_final_response)
        
        for i, code_block in enumerate(code_blocks):
            print(f"\nExecuting additional code block {i+1}:")
            print("```python")
            print(code_block)
            print("```")
            
            self.llm1.execute_code(code_block, lambda text: sys.stdout.write(text) or sys.stdout.flush())

def main():
    """Parse command line arguments and run the brain simulation"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Hemispherical Brain Simulation using two LLMs')
    parser.add_argument('--llm1', default='llama2', help='Model name for the left hemisphere (default: llama2)')
    parser.add_argument('--llm2', default='mistral', help='Model name for the right hemisphere (default: mistral)')
    parser.add_argument('--force', action='store_true', help='Force run without checking model availability')
    parser.add_argument('--list-models', action='store_true', help='List available models and exit')
    parser.add_argument('--no-terminal', action='store_true', help='Disable external terminal for the left hemisphere')
    
    args = parser.parse_args()
    
    # If just listing models
    if args.list_models:
        try:
            # Try both API endpoints
            print("Checking available models in Ollama...")
            
            # First method
            response = requests.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                models_data = response.json()
                print("\nModels (via /api/tags):")
                if "models" in models_data:
                    for model in models_data.get("models", []):
                        print(f"  - {model.get('name', 'Unknown')}")
                else:
                    print("  No models found with this endpoint.")
            
            # Second method
            response = requests.post("http://localhost:11434/api/list")
            if response.status_code == 200:
                models_list = response.json().get("models", [])
                print("\nModels (via /api/list):")
                for model in models_list:
                    print(f"  - {model.get('name', 'Unknown')}")
                    
            print("\nUse these model names with the --llm1 and --llm2 arguments.")
            return
        except Exception as e:
            print(f"Error listing models: {str(e)}")
            return
    
    # Create and run the simulation
    simulation = BrainSimulation(
        llm1_model=args.llm1, 
        llm2_model=args.llm2,
        use_external_terminal=not args.no_terminal
    )
    
    # Modify the run method to bypass model checking if --force is used
    if args.force:
        # We'll create a custom run method that skips model checking
        original_run = simulation.run
        
        def force_run():
            print("Forcing run without model availability check...")
            # Skip directly to the conversation loop
            print(f"Using models: {simulation.llm1.model_name} (Left Hemisphere) and {simulation.llm2.model_name} (Right Hemisphere)")
            
            while True:
                try:
                    # Get the user's query
                    user_query = input("\nYou: ")
                    
                    if user_query.lower() == 'exit':
                        print("Exiting the simulation. Goodbye!")
                        break
                    
                    # Run a conversation between the LLMs
                    simulation.run_conversation(user_query)
                except KeyboardInterrupt:
                    print("\nKeyboard interrupt detected. Exiting the simulation. Goodbye!")
                    break
                except Exception as e:
                    print(f"\nAn error occurred: {str(e)}")
                    print("The simulation will continue with the next query.")
        
        # Replace the run method with our custom version
        simulation.run = force_run
    
    # Run the simulation
    simulation.run()

if __name__ == "__main__":
    main()
